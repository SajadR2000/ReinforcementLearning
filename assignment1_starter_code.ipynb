{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "7V5QcOxwVWNm"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "nYRtpPvyVXAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb2ab377-5ddc-4d04-e904-c73ef1660cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"FrozenLake-v1\") # , is_slippery=False "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7EYL2HAawax"
      },
      "source": [
        "We will be using OpenAI Gym. In Gym, every environment has a state and action space, accessible via `env.action_space` and `env.observation_space`\n",
        "\n",
        "The underlying source code for the environment is available at: https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1u1mjJLaesX",
        "outputId": "c92c6636-e69f-42a7-b399-3fd8625a87d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action size: 4\n",
            "State size: 16\n"
          ]
        }
      ],
      "source": [
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space.n\n",
        "print(f\"Action size: {action_size}\")\n",
        "print(f\"State size: {state_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbqV2qrkVsS6",
        "outputId": "734e5a26-3c86-49f7-fad1-6f47771fcc22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(4)\n"
          ]
        }
      ],
      "source": [
        "# action space is represnted by a Discrete object\n",
        "print(env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "0lQdFgDKbUsl"
      },
      "outputs": [],
      "source": [
        "def get_random_trajectory(render=False):\n",
        "    ob = env.reset()\n",
        "    traj_length = 0\n",
        "    rewards = []\n",
        "    while True:\n",
        "        action = env.action_space.sample()\n",
        "        ob, reward, done, _ = env.step(action)\n",
        "        traj_length += 1\n",
        "        rewards.append(reward)\n",
        "        if done:\n",
        "            break\n",
        "        if render:\n",
        "            env.render()\n",
        "    return rewards, traj_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdcd9G5cbeyb",
        "outputId": "dc9ac777-34e8-4564-aaac-0e7817904f72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.624800000000369\n"
          ]
        }
      ],
      "source": [
        "mean_traj_length = 0\n",
        "n_episodes = 10000\n",
        "for _ in range(n_episodes):\n",
        "    rewards, traj_length  = get_random_trajectory(False)\n",
        "    mean_traj_length += 1 / n_episodes * traj_length\n",
        "    # print(np.sum(rewards))\n",
        "print(mean_traj_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlSJcEI80k_z"
      },
      "source": [
        "To implement value iteration and policy iteration, we need the underlying transition distributions. This is in general not available in Gym environments, but we can access it for FrozenLake.\n",
        "\n",
        "`env.env.P` is a dictionary containing the underlying transition and reward dynamics for the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pMSvV5cz0pK",
        "outputId": "174ae7bc-430c-45ad-f757-45aec5d17c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])\n"
          ]
        }
      ],
      "source": [
        "print(env.env.P.keys())\n",
        "transition_dict = env.env.P"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLldn2ph1eML"
      },
      "source": [
        "We can view the transition distribution for the initial state. Each key corresponds to an action, and the values give transition probabilities. The possible transitions are specified via tuples representing the probability of the transition, the next state, the reward, and whether the episode terminates.\n",
        "\n",
        "Actions are 0-indexed and correspond to [\"Left\", \"Down\", \"Right\", \"Up\"]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7YhYeeq0SZl",
        "outputId": "2ca8518d-4581-46f3-8a2b-e6587075377c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action 0: transitions (p, ns, r, d):  [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
            "action 1: transitions (p, ns, r, d):  [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True)]\n",
            "action 2: transitions (p, ns, r, d):  [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
            "action 3: transitions (p, ns, r, d):  [(0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n"
          ]
        }
      ],
      "source": [
        "init_state = transition_dict[14]\n",
        "actions = [\"Left\", \"Down\", \"Right\", \"Up\"]\n",
        "for k in init_state:\n",
        "    print(f\"action {k}: transitions (p, ns, r, d):  {init_state[k]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "7d_Jl9cL1FU7"
      },
      "outputs": [],
      "source": [
        "# helper function to print policy\n",
        "def print_policy(policy):\n",
        "    reshaped_policy = policy.reshape(4,4)\n",
        "    for i in range(4):\n",
        "        x = \" \"\n",
        "        for j in range(4):\n",
        "            x += actions[int(reshaped_policy[i][j])]\n",
        "            if j < 3:\n",
        "                x += \" | \"\n",
        "        print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPyTCQJLX5Te",
        "outputId": "742a31cf-ec76-464b-f252-bdbda72bf6b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random policy: \n",
            " Up | Up | Down | Down\n",
            " Right | Up | Up | Right\n",
            " Left | Down | Right | Up\n",
            " Down | Right | Right | Left\n"
          ]
        }
      ],
      "source": [
        "policy = np.random.randint(4, size=16)\n",
        "print(\"Random policy: \")\n",
        "print_policy(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVPR2oykXOmv"
      },
      "source": [
        "We have provided some possible functions and their signatures, though you are certainly free to modify as you see fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "7qmTwXakYh-m"
      },
      "outputs": [],
      "source": [
        "def value_iteration(values, gamma, iterations=100, termination=1e-4):\n",
        "    policy = np.zeros((16,), dtype=int)\n",
        "    for _ in range(iterations):\n",
        "        max_update = 0\n",
        "        # can make asynchronous updates to values\n",
        "        for i in range(16):\n",
        "            # ********** TODO ***********\n",
        "            v = values[i]\n",
        "            state_transition = transition_dict[i]\n",
        "            best_action = 0\n",
        "            best_action_return = -np.inf\n",
        "            for a in range(len(state_transition)):\n",
        "                return_a = 0\n",
        "                model_given_s_and_a = state_transition[a]\n",
        "                for j in range(len(model_given_s_and_a)):\n",
        "                    prob = model_given_s_and_a[j][0]\n",
        "                    next_state = model_given_s_and_a[j][1]\n",
        "                    reward = model_given_s_and_a[j][2]\n",
        "                    return_a += prob * (reward + gamma * values[next_state])\n",
        "                if return_a > best_action_return:\n",
        "                    best_action = a\n",
        "                    best_action_return = return_a\n",
        "            policy[i] = best_action\n",
        "            values[i] = best_action_return\n",
        "            max_update = max(max_update, abs(best_action_return - v))\n",
        "            # ********** TODO ***********\n",
        "        # terminate if values don't change much \n",
        "        # print(max_update)   \n",
        "        if max_update < termination:\n",
        "            break\n",
        "    return policy, values\n",
        "\n",
        "# estimate values\n",
        "def policy_evaluation(policy, init_values, gamma, termination=1e-4):\n",
        "    # ********** TODO ***********\n",
        "    values = init_values.copy()\n",
        "    while True:\n",
        "        max_update = 0\n",
        "        # can make asynchronous updates to values\n",
        "        for i in range(16):\n",
        "            # ********** TODO ***********\n",
        "            v = values[i]\n",
        "            state_transition = transition_dict[i]\n",
        "            a = policy[i]\n",
        "            return_a = 0\n",
        "            model_given_s_and_a = state_transition[a]\n",
        "            for j in range(len(model_given_s_and_a)):\n",
        "                prob = model_given_s_and_a[j][0]\n",
        "                next_state = model_given_s_and_a[j][1]\n",
        "                reward = model_given_s_and_a[j][2]\n",
        "                return_a += prob * (reward + gamma * values[next_state])\n",
        "            values[i] = return_a\n",
        "            max_update = max(max_update, abs(return_a - v))\n",
        "            # ********** TODO ***********\n",
        "        # terminate if values don't change much \n",
        "        # print(max_update)   \n",
        "        if max_update < termination:\n",
        "            break\n",
        "    return values\n",
        "\n",
        "# update actions\n",
        "def policy_improvement(values, gamma):\n",
        "    # ********** TODO ***********\n",
        "    policy = np.zeros((16,), dtype=int)\n",
        "    for i in range(16):\n",
        "        # ********** TODO ***********\n",
        "        state_transition = transition_dict[i]\n",
        "        best_action = 0\n",
        "        best_action_return = -np.inf\n",
        "        for a in range(len(state_transition)):\n",
        "            return_a = 0\n",
        "            model_given_s_and_a = state_transition[a]\n",
        "            for j in range(len(model_given_s_and_a)):\n",
        "                prob = model_given_s_and_a[j][0]\n",
        "                next_state = model_given_s_and_a[j][1]\n",
        "                reward = model_given_s_and_a[j][2]\n",
        "                return_a += prob * (reward + gamma * values[next_state])\n",
        "            if return_a > best_action_return:\n",
        "                best_action = a\n",
        "                best_action_return = return_a\n",
        "        policy[i] = best_action\n",
        "        \n",
        "    return policy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "qbyL025w3BNv"
      },
      "outputs": [],
      "source": [
        "policy, value = value_iteration(np.zeros((16,)), 0.9, iterations=100, termination=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_policy(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1_w8JXxcwVx",
        "outputId": "cf732f6c-49f2-4d14-a55d-f1769d477618"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Left | Up | Left | Up\n",
            " Left | Left | Left | Left\n",
            " Up | Down | Left | Left\n",
            " Left | Right | Down | Left\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(value.reshape((4,4)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3MVbW9fc0wm",
        "outputId": "3cae365d-d1c1-43ce-faf7-9d916d146453"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.06848032 0.06111567 0.07422254 0.05560469]\n",
            " [0.09153995 0.         0.11212558 0.        ]\n",
            " [0.14522151 0.24737863 0.29954442 0.        ]\n",
            " [0.         0.37986011 0.63898452 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(gamma=0.9):\n",
        "    old_policy = np.random.randint(4, size=16)\n",
        "    old_value = np.zeros((16,))\n",
        "    while True:\n",
        "        value = policy_evaluation(old_policy, old_value, gamma)\n",
        "        policy = policy_improvement(value, gamma)\n",
        "        if np.sum(policy != old_policy) == 0:\n",
        "            break\n",
        "        old_policy = policy\n",
        "        old_value = value\n",
        "    return policy, value\n"
      ],
      "metadata": {
        "id": "0lcIcWFVwBNS"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy, value = policy_iteration()"
      ],
      "metadata": {
        "id": "DBImzy6-q0tg"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_policy(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcMQdmA1q9MZ",
        "outputId": "324b82b5-631e-464c-d4d2-98353610fc12"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Left | Up | Left | Up\n",
            " Left | Left | Left | Left\n",
            " Up | Down | Left | Left\n",
            " Left | Right | Down | Left\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(value.reshape((4,4)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvhbFWZSq_Xy",
        "outputId": "8ecf0e1a-de81-4233-e08e-19ba6d4071de"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.06876448 0.0612918  0.07432063 0.05562446]\n",
            " [0.09175113 0.         0.11216899 0.        ]\n",
            " [0.14535878 0.24744876 0.29958535 0.        ]\n",
            " [0.         0.37990298 0.63900425 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(n_episodes=1000, \n",
        "               alpha=0.5, \n",
        "               epsilon=0.1, \n",
        "               epsilon_decay=True,\n",
        "               gamma=0.9,\n",
        "               render=False):\n",
        "    n_states = env.observation_space.n\n",
        "    n_actions = env.action_space.n\n",
        "    q = np.zeros((n_states, n_actions))\n",
        "\n",
        "    for i in range(n_episodes):\n",
        "        init_ob = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            rnd = np.random.uniform()\n",
        "            if rnd <= epsilon:\n",
        "                action = np.random.randint(4)\n",
        "            else:\n",
        "                state_q = q[init_ob, :]\n",
        "                a = np.where(state_q == max(state_q))[0]\n",
        "                action = a[np.random.randint(np.size(a))]\n",
        "            ob, reward, done, _ = env.step(action)\n",
        "            q[init_ob, action] = q[init_ob, action] +\\\n",
        "                alpha * (reward + gamma * max(q[ob, :]) - q[init_ob, action])\n",
        "            init_ob = ob\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "    return q"
      ],
      "metadata": {
        "id": "X1xuOK5NrPZN"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_policy(np.argmax(q_learning(), axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5idzP_MrdEe",
        "outputId": "dbb0b9d5-a309-4b8b-b350-efbdf3ae44b6"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Left | Up | Left | Up\n",
            " Left | Left | Left | Left\n",
            " Down | Down | Down | Left\n",
            " Left | Down | Right | Left\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3MMuGfapEcET"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}